{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001DCA848E270> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001DCA848ECF0> root_client=<openai.OpenAI object at 0x000001DCA72FF380> root_async_client=<openai.AsyncOpenAI object at 0x000001DCA848EA50> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Agentic AI** refers to artificial intelligence systems designed with agencyâ€”the capacity to act autonomously, make decisions, and pursue goals based on their programming and learned experiences. Unlike traditional AI systems that perform predefined tasks or follow explicit instructions, agentic AI possesses a degree of independence, allowing it to assess situations, set objectives, and determine the best course of action to achieve those objectives without constant human oversight.\\n\\n### Key Characteristics of Agentic AI\\n\\n1. **Autonomy**: Agentic AI systems operate independently, making decisions without requiring real-time human input. They can initiate actions based on their assessments of the environment and changing conditions.\\n\\n2. **Goal-Oriented Behavior**: These AI agents are designed to pursue specific goals or objectives. They can prioritize tasks, plan strategies, and adjust their actions to optimize outcomes.\\n\\n3. **Perception and Interpretation**: Agentic AI can perceive its environment through sensors or data inputs, interpret that information, and respond accordingly. This includes understanding complex data patterns, recognizing changes, and adapting to new information.\\n\\n4. **Learning and Adaptation**: Utilizing machine learning techniques, agentic AI can learn from experiences, improve its performance over time, and adapt to new scenarios without explicit reprogramming.\\n\\n5. **Interaction and Communication**: These AI systems can interact with other agents, systems, or humans, facilitating collaborative tasks, negotiation, and information exchange.\\n\\n### Applications of Agentic AI\\n\\n- **Autonomous Vehicles**: Self-driving cars use agentic AI to navigate roads, make real-time decisions in traffic, and respond to dynamic driving conditions.\\n\\n- **Robotics**: Robotic systems in manufacturing, healthcare, and service industries employ agentic AI to perform complex tasks, adapt to new tasks, and interact safely with humans and their environment.\\n\\n- **Virtual Assistants**: Advanced virtual assistants like Siri, Alexa, or more sophisticated enterprise solutions can manage schedules, perform tasks, and interact with other systems autonomously based on user preferences and behaviors.\\n\\n- **Financial Trading**: Agentic AI systems can execute trades, manage investment portfolios, and respond to market fluctuations without human intervention, aiming to optimize financial returns.\\n\\n- **Smart Grids and Energy Management**: AI agents manage energy distribution, optimize usage, and respond to demand changes in real-time, enhancing efficiency and sustainability.\\n\\n### Benefits of Agentic AI\\n\\n- **Efficiency and Productivity**: By automating complex decision-making processes, agentic AI can perform tasks faster and more accurately than humans, leading to increased productivity.\\n\\n- **Scalability**: These systems can manage large-scale operations and data processing, handling tasks that would be impractical for human teams.\\n\\n- **24/7 Operation**: Agentic AI can operate continuously without fatigue, ensuring consistent performance and availability.\\n\\n- **Personalization**: In consumer applications, agentic AI can tailor experiences and services to individual preferences, enhancing user satisfaction.\\n\\n### Challenges and Considerations\\n\\n- **Ethical Concerns**: The autonomy of agentic AI raises questions about accountability, decision-making transparency, and the potential for unintended consequences.\\n\\n- **Safety and Control**: Ensuring that agentic AI systems behave safely and as intended, especially in critical applications like healthcare or transportation, is paramount.\\n\\n- **Bias and Fairness**: AI agents learn from data that may contain biases, potentially leading to discriminatory behaviors or decisions if not properly managed.\\n\\n- **Regulation and Governance**: Developing frameworks to oversee the deployment and operation of agentic AI is essential to mitigate risks and ensure alignment with societal values.\\n\\n- **Dependence and Job Displacement**: Increased reliance on autonomous systems may impact employment in certain sectors, necessitating strategies for workforce transition and skill development.\\n\\n### Future Directions\\n\\nThe development of agentic AI is ongoing, with research focused on enhancing autonomy, improving learning algorithms, ensuring ethical compliance, and integrating these systems seamlessly into various aspects of society. Advances in areas like explainable AI, human-AI collaboration, and robust safety mechanisms are critical to realizing the full potential of agentic AI while addressing its challenges.\\n\\nIn summary, **Agentic AI** represents a significant advancement in artificial intelligence, characterized by autonomous decision-making and goal-oriented actions. Its applications span numerous industries, offering substantial benefits while also posing important ethical and practical challenges that must be carefully managed.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1199, 'prompt_tokens': 13, 'total_tokens': 1212, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_3da8b0b088', 'id': 'chatcmpl-Bln0qsSiLI4UgIm8jd8S91SuSIpih', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--3d562731-d826-4cae-907f-9fb10e84c53e-0' usage_metadata={'input_tokens': 13, 'output_tokens': 1199, 'total_tokens': 1212, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"Hi My name is Krish.\" I should start by greeting them back. Let me make sure to use their name so it feels personal. Maybe say something like \"Hello Krish! How can I assist you today?\" That\\'s friendly and opens the conversation. Wait, should I ask how I can help or just acknowledge their greeting? Probably better to offer assistance since they might need help with something. Yeah, that makes sense. Let me check for any typos. Nope, looks good. Alright, send that response.\\n</think>\\n\\nHello Krish! How can I assist you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 15, 'total_tokens': 139, 'completion_time': 0.284618626, 'prompt_time': 0.002717818, 'queue_time': 0.267210781, 'total_time': 0.287336444}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_28178d7ff6', 'finish_reason': 'stop', 'logprobs': None}, id='run--78804cf9-6b25-489a-819f-39377721d4e4-0', usage_metadata={'input_tokens': 15, 'output_tokens': 124, 'total_tokens': 139})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"Hi My name is Krish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001DCA8CFCA50>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001DCA8CFD6D0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001DCA8CFCA50>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001DCA8CFD6D0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is an open-source tool developed by the Hugging Face team specifically designed for **fine-tuning large language models (LLMs)**.  \n",
      "\n",
      "Here's a breakdown of what makes LangSmith noteworthy:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **User-Friendly Interface:**  LangSmith boasts a streamlined, web-based interface that simplifies the fine-tuning process, making it accessible to a wider range of users, even those without extensive machine learning expertise.\n",
      "* **Experiment Tracking:**  It provides robust experiment tracking capabilities, allowing you to meticulously monitor and compare the performance of different fine-tuning runs. This is crucial for iteratively improving your model.\n",
      "* **Dataset Management:** LangSmith streamlines dataset management, enabling you to easily upload, preprocess, and split your data for fine-tuning.\n",
      "* **Model Hub Integration:** It seamlessly integrates with the Hugging Face Model Hub, providing access to a vast library of pre-trained LLMs ready for fine-tuning. You can leverage these existing models and customize them for your specific tasks.\n",
      "* **Community-Driven:** As an open-source project, LangSmith benefits from a vibrant community of developers and researchers who contribute to its development, documentation, and support.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Faster Development Cycles:** By simplifying the fine-tuning process, LangSmith accelerates the development of customized LLMs.\n",
      "* **Improved Performance:**  Fine-tuning allows you to tailor LLMs to your specific domain or task, often resulting in significant performance gains.\n",
      "* **Accessibility:**  LangSmith democratizes access to powerful LLMs, empowering individuals and organizations without deep machine learning resources to leverage their capabilities.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "* **Text Generation:** Fine-tune LLMs for creative writing, summarization, dialogue systems, and more.\n",
      "* **Question Answering:**  Train LLMs to answer questions accurately and comprehensively from a specific knowledge base.\n",
      "* **Code Generation:**  Adapt LLMs to generate code in various programming languages.\n",
      "* **Sentiment Analysis:**  Fine-tune LLMs to classify text based on its emotional tone.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "* Visit the official LangSmith website ([https://www.philschmid.com/blog/2023-07-28-introducing-langsmith/](https://www.philschmid.com/blog/2023-07-28-introducing-langsmith/)) for detailed documentation and tutorials.\n",
      "* Explore the Hugging Face Model Hub ([https://huggingface.co/models](https://huggingface.co/models)) for pre-trained LLMs suitable for fine-tuning with LangSmith.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about LangSmith or how to get started with fine-tuning LLMs!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I can definitely tell you about Langsmith! \n",
      "\n",
      "Langsmith is an open-source tool developed by the **Hugging Face** team. It's designed to make it **easier to build and fine-tune large language models (LLMs)**, particularly within the context of **instruction-following tasks**.\n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "**1. User-Friendly Interface:** Langsmith provides a **web-based interface** that simplifies the process of interacting with LLMs. This makes it accessible to a wider range of users, even those without extensive coding experience.\n",
      "\n",
      "**2. Streamlined Workflow:** It streamlines the workflow of fine-tuning LLMs by offering **pre-built templates and pipelines**. This allows users to quickly adapt existing models to their specific use cases.\n",
      "\n",
      "**3. Instruction Tuning Focus:** Langsmith is particularly geared towards **instruction tuning**, a technique where LLMs are trained to follow instructions precisely. This is crucial for building LLMs that can be effectively used in real-world applications.\n",
      "\n",
      "**4. Open-Source and Collaborative:** As an open-source project, Langsmith encourages community contributions and collaboration. This fosters innovation and allows users to build upon the work of others.\n",
      "\n",
      "**5. Integration with Hugging Face Ecosystem:** Langsmith seamlessly integrates with the Hugging Face Hub, providing access to a vast library of pre-trained models and datasets. This makes it easy to find and utilize the resources needed for fine-tuning.\n",
      "\n",
      "**Here are some examples of what you can do with Langsmith:**\n",
      "\n",
      "* **Fine-tune a language model to answer questions based on a specific document.**\n",
      "* **Train a model to summarize text in a concise and informative way.**\n",
      "* **Create a chatbot that can engage in natural-sounding conversations.**\n",
      "* **Adapt a pre-trained model to generate code in a particular programming language.**\n",
      "\n",
      "**Overall, Langsmith is a powerful and versatile tool that empowers developers and researchers to harness the capabilities of LLMs for a wide range of applications.**\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or LLMs in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying machine learning models, specifically focusing on large language models (LLMs).', 'features': ['Modular and extensible design', 'Support for various LLM architectures', 'Tools for fine-tuning and evaluating models', 'Streamlined deployment pipeline', 'Community-driven development'], 'benefits': ['Accelerated LLM development cycle', 'Improved model performance through fine-tuning', 'Easy deployment and integration with applications', 'Transparency and accessibility through open-source nature'], 'website': 'https://github.com/langsmithai/langsmith'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee96082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed7d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"Langsmith is an open-source platform developed by the Weights & Biases (WandB) team. It simplifies the process of fine-tuning and deploying large language models (LLMs). \\n\\n  Here are some key features of Langsmith:\\n\\n  * **User-Friendly Interface:** Langsmith provides a web-based interface that makes it easy to interact with LLMs and manage your fine-tuning projects.\\n  * **Fine-Tuning Tools:** It offers tools and resources specifically designed for fine-tuning LLMs on your own datasets.\\n  * **Model Hub:** Langsmith has a growing library of pre-trained LLMs that you can explore and use as a starting point for your projects.\\n  * **Experiment Tracking:**  It integrates with WandB's powerful experiment tracking capabilities, allowing you to monitor your fine-tuning progress and compare different model configurations.\\n  * **Deployment Options:** Langsmith helps you deploy your fine-tuned LLMs as APIs or integrate them into your applications.\\n\\n  Essentially, Langsmith aims to democratize access to LLMs and make it easier for developers and researchers to leverage them for a wide range of applications.\"}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "940f704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<langsmith>\\n  <description>Langsmith is an open-source platform designed to help researchers and developers build and experiment with large language models (LLMs). </description>\\n  <features>\\n    <feature>Provides a modular and customizable framework for LLM development.</feature>\\n    <feature>Offers a variety of pre-trained LLMs and datasets.</feature>\\n    <feature>Supports fine-tuning and evaluation of LLMs.</feature>\\n    <feature>Facilitates collaboration and sharing of LLM research.</feature>\\n  </features>\\n  <purpose>\\n    <objective>To democratize access to LLM technology and accelerate research in the field.</objective>\\n  </purpose>\\n</langsmith>\\n```' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 162, 'prompt_tokens': 195, 'total_tokens': 357, 'completion_time': 0.294545455, 'prompt_time': 0.007782159, 'queue_time': 0.251103939, 'total_time': 0.302327614}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--b8619304-f24b-4eba-8bd1-01d042a9b22d-0' usage_metadata={'input_tokens': 195, 'output_tokens': 162, 'total_tokens': 357}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is a blockchain-based platform that aims to provide language learning services and opportunities for users. It leverages blockchain technology to create a secure and transparent ecosystem for language learners and educators to connect and exchange services.</answer></response>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 40, 'total_tokens': 91, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlnXNp8wq5Bf7tHTOgSGa4BpCGPiH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9d88896c-8569-4cb9-979a-0cb7fd98ba72-0' usage_metadata={'input_tokens': 40, 'output_tokens': 51, 'total_tokens': 91, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why was the math book sad? Because it had too many problems.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Splash</movie>\n",
      "<movie>Big</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>The Da Vinci Code</movie>\n",
      "<movie>Toy Story (franchise)</movie>\n",
      "<movie>Bridge of Spies</movie>\n",
      "<movie>Sully</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2999f98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
